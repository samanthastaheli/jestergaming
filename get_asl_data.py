# -*- coding: utf-8 -*-
"""gesture_recognizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb

Project: /mediapipe/_project.yaml
Book: /mediapipe/_book.yaml

<link rel="stylesheet" href="/mediapipe/site.css">

# Hand gesture recognition model customization guide

<table align="left" class="buttons">
  <td>
    <a href="https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb" target="_blank">
      <img src="https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png" alt="Colab logo"> Run in Colab
    </a>
  </td>

  <td>
    <a href="https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb" target="_blank">
      <img src="https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png" alt="GitHub logo">
      View on GitHub
    </a>
  </td>
</table>
"""

#@title License information
# Copyright 2023 The MediaPipe Authors.
# Licensed under the Apache License, Version 2.0 (the "License");
#
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.

This notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the [HaGRID](https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p) dataset.

## Prerequisites

Install the MediaPipe Model Maker package.
"""

# !pip install --upgrade pip
# !pip install mediapipe-model-maker

"""Import the required libraries."""

# from google.colab import files
import os
# import tensorflow as tf
# assert tf.__version__.startswith('2')

# from mediapipe_model_maker import gesture_recognizer

# import matplotlib.pyplot as plt
import pandas as pd
import kagglehub
import shutil 

"""## Simple End-to-End Example

This end-to-end example uses Model Maker to customize a model for on-device gesture recognition.

### Get the dataset

The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures.

This example uses a rock paper scissors dataset sample which is downloaded from GCS.
"""


def get_asl_data():

  # Download latest version
  path = kagglehub.dataset_download("grassknoted/asl-alphabet")

  print("Path to dataset files:", path)

  os.makedirs("data", exist_ok=True)

  #files needed (R, J, M, T, )
  folders_needed = {"R":"Action", "J":"Journal", "M":"Map", "T":"Toolbar"}

  for asl_folders in os.listdir(f"{path}/asl_alphabet_train/asl_alphabet_train"):
    if asl_folders in folders_needed.keys():
      print(f"\033[95mMoving images in {asl_folders} to {folders_needed[asl_folders]}.\033[0m")

      src_path = f"{path}/asl_alphabet_train/asl_alphabet_train/{asl_folders}/"
      dst_path = f"data/{folders_needed[asl_folders]}"

      os.makedirs(dst_path, exist_ok=True)
      
      # Move every file in src_path
      for file in os.listdir(src_path):
          src_file = os.path.join(src_path, file)
          dst_file = os.path.join(dst_path, file)
          shutil.move(src_file, dst_file)


if __name__ == "__main__":
  get_asl_data()